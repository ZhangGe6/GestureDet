{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLTPoltter():\n",
    "    def __init__(self, ax_size, **kwargs):\n",
    "        self.fig, self.ax = plt.subplots(*ax_size, **kwargs)\n",
    "\n",
    "    def plt_show_cv2_img(self, cv2_img, ax_id):\n",
    "        self.ax[ax_id].imshow(cv2_img[:, :, ::-1])\n",
    "\n",
    "    def plt_show_np_single_channel(self, np_array, channel, ax_id):\n",
    "        array = np_array[channel]\n",
    "        self.ax[ax_id].imshow(array)\n",
    "\n",
    "    def plt_show_np_concat(self, np_array, concat_num, ax_id):\n",
    "        channel, height, wdith = np_array.shape\n",
    "        concat = np.zeros((height, wdith))\n",
    "        for c in range(concat_num):\n",
    "            # print(np.max(np_array[c]))\n",
    "            concat += np_array[c]\n",
    "        self.ax[ax_id].imshow(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_target_mask(joints, model_input_size, model_output_size):\n",
    "    sigma = 1\n",
    "    r_size = sigma * 2\n",
    "    keypoints_num = 21\n",
    "    output_height, output_width = model_output_size\n",
    "    feat_stride = np.array(model_input_size) / np.array(model_output_size)\n",
    "    target_mask_padded = np.zeros((keypoints_num, output_height+2*r_size, output_width+2*r_size), dtype=np.float32)\n",
    "    target_weight = np.ones((keypoints_num, 1), dtype=np.float32)\n",
    "    for i, pt in enumerate(joints):\n",
    "        x, y = pt\n",
    "        # print(x, y)\n",
    "        mu_y = int(y / feat_stride[0] + 0.5)\n",
    "        mu_x = int(x / feat_stride[1] + 0.5)\n",
    "        # print(mu_x, mu_y) \n",
    "\n",
    "        # the second `+ r_size` means pad by r_rize\n",
    "        patch_ymin, patch_xmin = mu_y - r_size + r_size, mu_x - r_size + r_size\n",
    "        patch_ymax, patch_xmax = mu_y + r_size + 1 + r_size, mu_x + r_size + 1 + r_size\n",
    "        if patch_ymin < 0 or patch_xmin < 0 or patch_ymax >= output_height or patch_xmax >= output_width:\n",
    "            target_weight[i] = 0   # this can handle some wrong annotations like `Ricki_unit_8.flv_000002_l`\n",
    "\n",
    "        size = 2 * r_size + 1\n",
    "        mesh_x = np.arange(0, size, 1, np.float32)\n",
    "        mesh_y = mesh_x[:, np.newaxis]\n",
    "        mesh_x0 = mesh_y0 = size // 2\n",
    "        patch_guassian = np.exp(-((mesh_x - mesh_x0) ** 2 + (mesh_y - mesh_y0) ** 2) / (2 * (sigma ** 2)))\n",
    "        # print(patch_guassian.shape)  # (7, 7)\n",
    "        # print(target_mask_padded[i].shape) # (262, 198)\n",
    "        # print(patch_ymin, patch_ymax, patch_xmin, patch_xmax)  # 271 278 75 82\n",
    "        # print((target_mask_padded[i][patch_ymin:patch_ymax, patch_xmin:patch_xmax].shape))  # (0, 7)\n",
    "        # print(target_weight[i])\n",
    "        if target_weight[i] > 0.5:\n",
    "            target_mask_padded[i][patch_ymin:patch_ymax, patch_xmin:patch_xmax] = patch_guassian\n",
    "        target_mask = target_mask_padded[:, r_size:-r_size, r_size:-r_size]\n",
    "        \n",
    "    return target_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_img_path = '/home/zg/wdir/zg/moyu/GestureDet/datasets/CMU_PANOPTIC/hand_labels/manual_train/024366101_01_l.jpg'\n",
    "json_path = '/home/zg/wdir/zg/moyu/GestureDet/datasets/CMU_PANOPTIC/hand_labels/manual_train/024366101_01_l.json'\n",
    "\n",
    "full_img = cv2.imread(full_img_path)\n",
    "label = json.load(open(json_path))\n",
    "full_height, full_width, _ = full_img.shape\n",
    "joints = label['hand_pts']\n",
    "# print(full_img.shape)\n",
    "# print(label['hand_pts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_utils.transforms import get_bbox_from_joints\n",
    "hand_bbox = get_bbox_from_joints(joints, full_height, full_width, MARGIN=15)\n",
    "xmin, ymin, xmax, ymax = hand_bbox\n",
    "bbox_img = copy.deepcopy(full_img)\n",
    "bbox_img = cv2.rectangle(bbox_img, (xmin, ymin), (xmax, ymax), color=(0, 0, 255), thickness=3)\n",
    "for pt in joints:\n",
    "    x, y, _ = pt\n",
    "    bbox_img = cv2.circle(bbox_img, center=(int(x), int(y)), radius=2, color=(255, 255, 255), thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1581.5  490. ] [148.75 148.75]\n"
     ]
    }
   ],
   "source": [
    "from pose_utils.transforms import get_affine_transform, _box_to_center_scale, affine_transform\n",
    "model_input_size = 256, 192\n",
    "model_input_height, model_input_width = model_input_size\n",
    "model_output_size = 64, 48\n",
    "model_output_height, model_output_width = model_output_size\n",
    "\n",
    "center, scale = _box_to_center_scale(xmin, ymin, xmax-xmin, ymax-ymin)#, aspect_ratio=1, scale_mult=1)\n",
    "print(center, scale)\n",
    "trans = get_affine_transform(center, scale, rot=0, output_size=(model_input_width, model_input_height), inv=0)\n",
    "# print(trans)\n",
    "to_affine_img = copy.deepcopy(full_img)\n",
    "affined_img = cv2.warpAffine(to_affine_img, trans, (model_input_width, model_input_height), flags=cv2.INTER_LINEAR)\n",
    "affined_img_bk = copy.deepcopy(affined_img)\n",
    "\n",
    "affined_joints = np.zeros((len(joints), 2))\n",
    "for i in range(len(joints)):\n",
    "    affined_joints[i][0:2] = affine_transform(joints[i][0:2], trans)\n",
    "for affined_pt in affined_joints:\n",
    "    x, y = affined_pt\n",
    "    affined_img = cv2.circle(affined_img, center=(int(x), int(y)), radius=2, color=(255, 255, 255), thickness=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = gen_target_mask(affined_joints, model_input_size=model_input_size, model_output_size=model_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_utils.transforms import heatmap_to_coord_simple\n",
    "preds, maxvals = heatmap_to_coord_simple(target_mask, hand_bbox)\n",
    "gt_recover_joint_img = copy.deepcopy(full_img)\n",
    "for pt in preds:\n",
    "    x, y = pt\n",
    "    gt_recover_joint_img = cv2.circle(gt_recover_joint_img, center=(int(x), int(y)), radius=2, color=(255, 255, 255), thickness=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_poltter = PLTPoltter(ax_size=(1, 3), figsize=(20, 10))\n",
    "# plt_poltter.plt_show_cv2_img(bbox_img, 0)\n",
    "plt_poltter.plt_show_cv2_img(affined_img, 0)\n",
    "# plt_poltter.plt_show_np_single_channel(target_mask, channel=13, ax_id=1)\n",
    "plt_poltter.plt_show_np_concat(target_mask, concat_num=21, ax_id=2)\n",
    "plt_poltter.plt_show_cv2_img(gt_recover_joint_img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UltraLightSimplePoseNet\n",
    "from pose_utils.transforms import to_tensor, norm\n",
    "weight_path = './checkpoints/copy_mobilenetv2_epoch_75_acc1_0.70.pt'\n",
    "pose_estimator = UltraLightSimplePoseNet().cuda()\n",
    "pose_estimator.load_state_dict(torch.load(weight_path))\n",
    "affined_img_bk = to_tensor(affined_img_bk)\n",
    "affined_img_bk = norm(affined_img_bk, (-0.406, -0.457, -0.480))\n",
    "affined_img_bk = affined_img_bk.unsqueeze(0).cuda()\n",
    "pred_mask = pose_estimator(affined_img_bk)\n",
    "pred_mask_numpy = pred_mask[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_utils.transforms import heatmap_to_coord_simple\n",
    "preds, maxvals = heatmap_to_coord_simple(pred_mask_numpy, hand_bbox)\n",
    "pred_recover_joint_img = copy.deepcopy(full_img)\n",
    "for pt in preds:\n",
    "    x, y = pt\n",
    "    pred_recover_joint_img = cv2.circle(pred_recover_joint_img, center=(int(x), int(y)), radius=2, color=(255, 255, 255), thickness=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt_poltter = PLTPoltter(ax_size=(1, 2), figsize=(20, 10))\n",
    "plt_poltter.plt_show_cv2_img(affined_img, 0)   # drawn with gt\n",
    "see_channel = 5\n",
    "# plt_poltter.plt_show_np_single_channel(pred_mask_numpy, channel=see_channel, ax_id=1)\n",
    "# plt_poltter.plt_show_np_single_channel(target_mask, channel=see_channel, ax_id=2)\n",
    "# plt_poltter.plt_show_np_concat(pred_mask_numpy, concat_num=21, ax_id=2)\n",
    "plt_poltter.plt_show_cv2_img(pred_recover_joint_img, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e0647ecf0dfce2c803e6d23827cf1b72549446a90a5c8f13f4fbc3972468ba5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('mc': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
